{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run 1 100K (Europarl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corpus corpus_1's weight should be given. We default it to 1 for you.\n",
      "[2024-05-29 06:55:22,711 INFO] Counter vocab from 10000 samples.\n",
      "[2024-05-29 06:55:22,711 INFO] Build vocab on 10000 transformed examples/corpus.\n",
      "[2024-05-29 06:55:29,725 INFO] Counters src: 24423\n",
      "[2024-05-29 06:55:29,725 INFO] Counters tgt: 37503\n"
     ]
    }
   ],
   "source": [
    "!onmt_build_vocab -config run1_en_fr.yaml -n_sample 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-05-29 10:13:29,191 INFO] Missing transforms field for corpus_1 data, set to default: [].\n",
      "[2024-05-29 10:13:29,191 WARNING] Corpus corpus_1's weight should be given. We default it to 1 for you.\n",
      "[2024-05-29 10:13:29,191 INFO] Missing transforms field for valid data, set to default: [].\n",
      "[2024-05-29 10:13:29,192 INFO] Parsed 2 corpora from -data.\n",
      "[2024-05-29 10:13:29,192 INFO] Get special vocabs from Transforms: {'src': [], 'tgt': []}.\n",
      "[2024-05-29 10:13:29,291 INFO] The first 10 tokens of the vocabs are:['<unk>', '<blank>', '<s>', '</s>', 'the', ',', 'be', '.', 'of', 'to']\n",
      "[2024-05-29 10:13:29,291 INFO] The decoder start token is: <s>\n",
      "[2024-05-29 10:13:29,291 INFO] Building model...\n",
      "[2024-05-29 10:13:29,740 INFO] Switching model to float32 for amp/apex_amp\n",
      "[2024-05-29 10:13:29,741 INFO] Non quantized layer compute is fp32\n",
      "[2024-05-29 10:13:29,991 INFO] NMTModel(\n",
      "  (encoder): TransformerEncoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(24432, 512, padding_idx=1)\n",
      "        )\n",
      "        (pe): PositionalEncoding()\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): ModuleList(\n",
      "      (0-5): 6 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=512, out_features=2048, bias=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "  )\n",
      "  (decoder): TransformerDecoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(32768, 512, padding_idx=1)\n",
      "        )\n",
      "        (pe): PositionalEncoding()\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "    (transformer_layers): ModuleList(\n",
      "      (0-5): 6 x TransformerDecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=512, out_features=2048, bias=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (context_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "        )\n",
      "        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (generator): Linear(in_features=512, out_features=32768, bias=True)\n",
      ")\n",
      "[2024-05-29 10:13:29,993 INFO] encoder: 31396864\n",
      "[2024-05-29 10:13:29,993 INFO] decoder: 58772480\n",
      "[2024-05-29 10:13:29,993 INFO] * number of parameters: 90169344\n",
      "[2024-05-29 10:13:29,994 INFO] Trainable parameters = {'torch.float32': 90169344, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}\n",
      "[2024-05-29 10:13:29,994 INFO] Non trainable parameters = {'torch.float32': 0, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}\n",
      "[2024-05-29 10:13:29,994 INFO]  * src vocab size = 24432\n",
      "[2024-05-29 10:13:29,994 INFO]  * tgt vocab size = 32768\n",
      "[2024-05-29 10:13:29,997 INFO] Starting training on GPU: [0]\n",
      "[2024-05-29 10:13:29,997 INFO] Start training loop and validate every 500 steps...\n",
      "[2024-05-29 10:13:29,997 INFO] Scoring with: TransformPipe()\n",
      "[2024-05-29 10:13:33,331 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 1\n",
      "[2024-05-29 10:13:36,604 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 1\n",
      "[2024-05-29 10:13:36,665 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 2\n",
      "[2024-05-29 10:13:39,184 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 2\n",
      "[2024-05-29 10:13:41,743 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 3\n",
      "[2024-05-29 10:13:44,331 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 4\n",
      "[2024-05-29 10:13:46,904 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 5\n",
      "[2024-05-29 10:13:49,479 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 6\n",
      "[2024-05-29 10:13:52,092 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 7\n",
      "[2024-05-29 10:13:54,659 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 8\n",
      "[2024-05-29 10:13:57,251 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 9\n",
      "[2024-05-29 10:13:59,839 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 10\n",
      "[2024-05-29 10:14:02,440 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 11\n",
      "[2024-05-29 10:14:05,035 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 12\n",
      "[2024-05-29 10:14:07,613 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 13\n",
      "[2024-05-29 10:14:10,222 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 14\n",
      "[2024-05-29 10:14:12,920 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 15\n",
      "[2024-05-29 10:14:15,631 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 16\n",
      "[2024-05-29 10:14:18,717 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 17\n",
      "[2024-05-29 10:14:21,409 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 18\n",
      "[2024-05-29 10:14:24,576 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 19\n",
      "[2024-05-29 10:14:27,237 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 20\n",
      "[2024-05-29 10:14:29,982 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 21\n",
      "[2024-05-29 10:14:33,127 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 22\n",
      "[2024-05-29 10:14:35,846 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 23\n",
      "[2024-05-29 10:14:39,261 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 24\n",
      "[2024-05-29 10:14:42,628 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 25\n",
      "[2024-05-29 10:16:40,309 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 26\n",
      "[2024-05-29 10:16:43,184 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 27\n",
      "[2024-05-29 10:16:45,924 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 28\n",
      "[2024-05-29 10:16:48,769 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 29\n",
      "[2024-05-29 10:16:51,695 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 30\n",
      "[2024-05-29 10:16:54,803 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 31\n",
      "[2024-05-29 10:16:57,576 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 32\n",
      "[2024-05-29 10:17:00,792 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 33\n",
      "[2024-05-29 10:17:03,840 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 34\n",
      "[2024-05-29 10:17:06,670 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 35\n",
      "[2024-05-29 10:17:09,348 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 36\n",
      "[2024-05-29 10:17:12,076 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 37\n",
      "[2024-05-29 10:17:14,877 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 38\n",
      "[2024-05-29 10:17:18,205 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 39\n",
      "[2024-05-29 10:17:21,375 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 40\n",
      "[2024-05-29 10:17:24,087 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 41\n",
      "[2024-05-29 10:17:26,773 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 42\n",
      "[2024-05-29 10:17:30,297 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 43\n",
      "[2024-05-29 10:17:33,513 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 44\n",
      "[2024-05-29 10:17:36,340 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 45\n",
      "[2024-05-29 10:17:39,048 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 46\n",
      "[2024-05-29 10:17:41,713 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 47\n",
      "[2024-05-29 10:17:45,013 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 48\n",
      "[2024-05-29 10:17:48,379 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 49\n",
      "[2024-05-29 10:17:51,219 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 50\n",
      "[2024-05-29 10:17:53,945 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 51\n",
      "[2024-05-29 10:17:56,666 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 52\n",
      "[2024-05-29 10:17:59,350 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 53\n",
      "[2024-05-29 10:18:02,593 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 54\n",
      "[2024-05-29 10:18:06,069 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 55\n",
      "[2024-05-29 10:18:08,844 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 56\n",
      "[2024-05-29 10:18:11,526 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 57\n",
      "[2024-05-29 10:18:14,241 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 58\n",
      "[2024-05-29 10:18:16,954 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 59\n",
      "[2024-05-29 10:18:20,066 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 60\n",
      "[2024-05-29 10:18:23,563 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 61\n",
      "[2024-05-29 10:18:26,793 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 62\n",
      "[2024-05-29 10:18:29,595 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 63\n",
      "[2024-05-29 10:18:32,223 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 64\n",
      "[2024-05-29 10:18:34,933 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 65\n",
      "[2024-05-29 10:18:37,805 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 66\n",
      "[2024-05-29 10:18:41,235 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 67\n",
      "[2024-05-29 10:18:43,975 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 68\n",
      "[2024-05-29 10:18:46,647 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 69\n",
      "[2024-05-29 10:18:49,346 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 70\n",
      "[2024-05-29 10:18:52,435 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 71\n",
      "[2024-05-29 10:18:55,134 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 72\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Shen\\miniconda3\\envs\\TAA\\lib\\runpy.py\", line 194, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\Shen\\miniconda3\\envs\\TAA\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\Shen\\miniconda3\\envs\\TAA\\Scripts\\onmt_train.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\Shen\\miniconda3\\envs\\TAA\\lib\\site-packages\\onmt\\bin\\train.py\", line 67, in main\n",
      "    train(opt)\n",
      "  File \"C:\\Users\\Shen\\miniconda3\\envs\\TAA\\lib\\site-packages\\onmt\\bin\\train.py\", line 52, in train\n",
      "    train_process(opt, device_id=0)\n",
      "  File \"C:\\Users\\Shen\\miniconda3\\envs\\TAA\\lib\\site-packages\\onmt\\train_single.py\", line 238, in main\n",
      "    trainer.train(\n",
      "  File \"C:\\Users\\Shen\\miniconda3\\envs\\TAA\\lib\\site-packages\\onmt\\trainer.py\", line 308, in train\n",
      "    for i, (batches, normalization) in enumerate(self._accum_batches(train_iter)):\n",
      "  File \"C:\\Users\\Shen\\miniconda3\\envs\\TAA\\lib\\site-packages\\onmt\\trainer.py\", line 238, in _accum_batches\n",
      "    for batch, bucket_idx in iterator:\n",
      "  File \"C:\\Users\\Shen\\miniconda3\\envs\\TAA\\lib\\site-packages\\onmt\\inputters\\dynamic_iterator.py\", line 373, in __iter__\n",
      "    for (tensor_batch, bucket_idx) in self.data_iter:\n",
      "  File \"C:\\Users\\Shen\\miniconda3\\envs\\TAA\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 633, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"C:\\Users\\Shen\\miniconda3\\envs\\TAA\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1345, in _next_data\n",
      "    return self._process_data(data)\n",
      "  File \"C:\\Users\\Shen\\miniconda3\\envs\\TAA\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1371, in _process_data\n",
      "    data.reraise()\n",
      "  File \"C:\\Users\\Shen\\miniconda3\\envs\\TAA\\lib\\site-packages\\torch\\_utils.py\", line 644, in reraise\n",
      "    raise exception\n",
      "MemoryError: Caught MemoryError in DataLoader worker process 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Shen\\miniconda3\\envs\\TAA\\lib\\site-packages\\onmt\\inputters\\dynamic_iterator.py\", line 85, in __iter__\n",
      "    item = next(iterator)\n",
      "StopIteration\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Shen\\miniconda3\\envs\\TAA\\lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 308, in _worker_loop\n",
      "    data = fetcher.fetch(index)\n",
      "  File \"C:\\Users\\Shen\\miniconda3\\envs\\TAA\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 41, in fetch\n",
      "    data = next(self.dataset_iter)\n",
      "  File \"C:\\Users\\Shen\\miniconda3\\envs\\TAA\\lib\\site-packages\\onmt\\inputters\\dynamic_iterator.py\", line 341, in __iter__\n",
      "    for bucket, bucket_idx in self._bucketing():\n",
      "  File \"C:\\Users\\Shen\\miniconda3\\envs\\TAA\\lib\\site-packages\\onmt\\inputters\\dynamic_iterator.py\", line 275, in _bucketing\n",
      "    for ex in self.mixer:\n",
      "  File \"C:\\Users\\Shen\\miniconda3\\envs\\TAA\\lib\\site-packages\\onmt\\inputters\\dynamic_iterator.py\", line 89, in __iter__\n",
      "    item = next(iterator)\n",
      "  File \"C:\\Users\\Shen\\miniconda3\\envs\\TAA\\lib\\site-packages\\onmt\\inputters\\text_corpus.py\", line 281, in __iter__\n",
      "    yield from corpus\n",
      "  File \"C:\\Users\\Shen\\miniconda3\\envs\\TAA\\lib\\site-packages\\onmt\\inputters\\text_corpus.py\", line 240, in _process\n",
      "    for i, example in enumerate(stream):\n",
      "  File \"C:\\Users\\Shen\\miniconda3\\envs\\TAA\\lib\\site-packages\\onmt\\inputters\\text_corpus.py\", line 160, in load\n",
      "    yield make_ex(sline.decode(\"utf-8\"), tline, align)\n",
      "  File \"C:\\Users\\Shen\\miniconda3\\envs\\TAA\\lib\\site-packages\\onmt\\inputters\\text_corpus.py\", line 120, in make_ex\n",
      "    sline, sfeats = parse_features(\n",
      "  File \"C:\\Users\\Shen\\miniconda3\\envs\\TAA\\lib\\site-packages\\onmt\\inputters\\text_utils.py\", line 16, in parse_features\n",
      "    for token in line.split(\" \"):\n",
      "MemoryError\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!onmt_train -config run1_en_fr.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!onmt_translate -model ../data/model/run1/model_step_1000.pt -src ../data/raw/Europarl/Europarl_test_500.en -output ../data/result/pred1_Europarl_1000.txt -gpu 0 -verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!onmt_translate -model ../data/model/run1/model_step_1000.pt -src ../data/raw/Emea/Emea_test_500.en -output ../data/result/pred1_Emea_1000.txt -gpu 0 -verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "\n",
    "with open(\"../data/result/pred1_Europarl_1000.txt\", \"r\", encoding=\"utf-8\") as pred_file:\n",
    "    translations = pred_file.readlines()\n",
    "with open(\"../data/raw/Europarl/Europarl_test_500.fr\", \"r\", encoding=\"utf-8\") as ref_file:\n",
    "    references = ref_file.readlines()\n",
    "references = [ref.strip().split() for ref in references]\n",
    "translations = [trans.strip().split() for trans in translations]\n",
    "bleu_score = nltk.translate.bleu_score.corpus_bleu(references, translations, smoothing_function=nltk.translate.bleu_score.SmoothingFunction().method1)\n",
    "print(\"BLEU Score run 1 Europarl:\", bleu_score)\n",
    "\n",
    "\n",
    "with open(\"../data/result/pred1_Emea_1000.txt\", \"r\", encoding=\"utf-8\") as pred_file:\n",
    "    translations = pred_file.readlines()\n",
    "with open(\"../data/raw/Emea/Emea_test_500.fr\", \"r\", encoding=\"utf-8\") as ref_file:\n",
    "    references = ref_file.readlines()\n",
    "references = [ref.strip().split() for ref in references]\n",
    "translations = [trans.strip().split() for trans in translations]\n",
    "bleu_score = nltk.translate.bleu_score.corpus_bleu(references, translations, smoothing_function=nltk.translate.bleu_score.SmoothingFunction().method1)\n",
    "print(\"BLEU Score run 1 Emea:\", bleu_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run 2 100K+10K (Europarl+Emea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corpus corpus_1's weight should be given. We default it to 1 for you.\n",
      "Corpus corpus_2's weight should be given. We default it to 1 for you.\n",
      "[2024-05-29 06:08:44,936 INFO] Counter vocab from 10000 samples.\n",
      "[2024-05-29 06:08:44,936 INFO] Build vocab on 10000 transformed examples/corpus.\n",
      "[2024-05-29 06:08:52,274 INFO] Counters src: 25651\n",
      "[2024-05-29 06:08:52,274 INFO] Counters tgt: 38916\n"
     ]
    }
   ],
   "source": [
    "!onmt_build_vocab -config run2_en_fr.yaml -n_sample 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-05-29 07:16:32,051 INFO] Missing transforms field for corpus_1 data, set to default: [].\n",
      "[2024-05-29 07:16:32,051 WARNING] Corpus corpus_1's weight should be given. We default it to 1 for you.\n",
      "[2024-05-29 07:16:32,051 INFO] Missing transforms field for corpus_2 data, set to default: [].\n",
      "[2024-05-29 07:16:32,052 WARNING] Corpus corpus_2's weight should be given. We default it to 1 for you.\n",
      "[2024-05-29 07:16:32,052 INFO] Missing transforms field for valid data, set to default: [].\n",
      "[2024-05-29 07:16:32,052 INFO] Parsed 3 corpora from -data.\n",
      "[2024-05-29 07:16:32,053 INFO] Get special vocabs from Transforms: {'src': [], 'tgt': []}.\n",
      "[2024-05-29 07:16:32,163 INFO] The first 10 tokens of the vocabs are:['<unk>', '<blank>', '<s>', '</s>', 'the', ',', 'be', '.', 'of', 'to']\n",
      "[2024-05-29 07:16:32,163 INFO] The decoder start token is: <s>\n",
      "[2024-05-29 07:16:32,163 INFO] Building model...\n",
      "[2024-05-29 07:16:32,412 INFO] Switching model to float32 for amp/apex_amp\n",
      "[2024-05-29 07:16:32,412 INFO] Non quantized layer compute is fp32\n",
      "[2024-05-29 07:16:32,788 INFO] NMTModel(\n",
      "  (encoder): RNNEncoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(25656, 500, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "      (dropout): Dropout(p=0.3, inplace=False)\n",
      "    )\n",
      "    (rnn): LSTM(500, 500, num_layers=2, batch_first=True, dropout=0.3)\n",
      "  )\n",
      "  (decoder): InputFeedRNNDecoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(32768, 500, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "      (dropout): Dropout(p=0.3, inplace=False)\n",
      "    )\n",
      "    (dropout): Dropout(p=0.3, inplace=False)\n",
      "    (rnn): StackedLSTM(\n",
      "      (dropout): Dropout(p=0.3, inplace=False)\n",
      "      (layers): ModuleList(\n",
      "        (0): LSTMCell(1000, 500)\n",
      "        (1): LSTMCell(500, 500)\n",
      "      )\n",
      "    )\n",
      "    (attn): GlobalAttention(\n",
      "      (linear_in): Linear(in_features=500, out_features=500, bias=False)\n",
      "      (linear_out): Linear(in_features=1000, out_features=500, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (generator): Linear(in_features=500, out_features=32768, bias=True)\n",
      ")\n",
      "[2024-05-29 07:16:32,789 INFO] encoder: 16836000\n",
      "[2024-05-29 07:16:32,789 INFO] decoder: 38558768\n",
      "[2024-05-29 07:16:32,789 INFO] * number of parameters: 55394768\n",
      "[2024-05-29 07:16:32,789 INFO] Trainable parameters = {'torch.float32': 55394768, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}\n",
      "[2024-05-29 07:16:32,789 INFO] Non trainable parameters = {'torch.float32': 0, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}\n",
      "[2024-05-29 07:16:32,789 INFO]  * src vocab size = 25656\n",
      "[2024-05-29 07:16:32,789 INFO]  * tgt vocab size = 32768\n",
      "[2024-05-29 07:16:32,791 INFO] Starting training on GPU: [0]\n",
      "[2024-05-29 07:16:32,791 INFO] Start training loop and validate every 500 steps...\n",
      "[2024-05-29 07:16:32,791 INFO] Scoring with: TransformPipe()\n",
      "[2024-05-29 07:16:36,038 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 1\n",
      "[2024-05-29 07:16:36,039 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 1\n",
      "\t\t\t* corpus_2: 1\n",
      "[2024-05-29 07:16:39,303 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 1\n",
      "[2024-05-29 07:16:39,303 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 1\n",
      "\t\t\t* corpus_2: 1\n",
      "[2024-05-29 07:16:39,364 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 2\n",
      "\t\t\t* corpus_2: 1\n",
      "[2024-05-29 07:16:42,035 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 2\n",
      "\t\t\t* corpus_2: 1\n",
      "[2024-05-29 07:16:44,602 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 2\n",
      "\t\t\t* corpus_2: 2\n",
      "[2024-05-29 07:16:44,750 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 3\n",
      "\t\t\t* corpus_2: 2\n",
      "[2024-05-29 07:16:47,358 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 3\n",
      "\t\t\t* corpus_2: 3\n",
      "[2024-05-29 07:16:47,506 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 4\n",
      "\t\t\t* corpus_2: 3\n",
      "[2024-05-29 07:16:50,125 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 4\n",
      "\t\t\t* corpus_2: 4\n",
      "[2024-05-29 07:16:50,272 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 5\n",
      "\t\t\t* corpus_2: 4\n",
      "[2024-05-29 07:16:52,908 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 5\n",
      "\t\t\t* corpus_2: 5\n",
      "[2024-05-29 07:16:53,053 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 6\n",
      "\t\t\t* corpus_2: 5\n",
      "[2024-05-29 07:16:55,735 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 6\n",
      "\t\t\t* corpus_2: 6\n",
      "[2024-05-29 07:16:55,890 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 7\n",
      "\t\t\t* corpus_2: 6\n",
      "[2024-05-29 07:16:58,520 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 7\n",
      "\t\t\t* corpus_2: 7\n",
      "[2024-05-29 07:16:58,669 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 8\n",
      "\t\t\t* corpus_2: 7\n",
      "[2024-05-29 07:17:01,297 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 8\n",
      "\t\t\t* corpus_2: 8\n",
      "[2024-05-29 07:17:01,443 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 9\n",
      "\t\t\t* corpus_2: 8\n",
      "[2024-05-29 07:17:04,093 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 9\n",
      "\t\t\t* corpus_2: 9\n",
      "[2024-05-29 07:17:05,179 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 10\n",
      "\t\t\t* corpus_2: 9\n",
      "[2024-05-29 07:17:07,779 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 10\n",
      "\t\t\t* corpus_2: 10\n",
      "[2024-05-29 07:17:07,922 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 11\n",
      "\t\t\t* corpus_2: 10\n",
      "[2024-05-29 07:17:10,516 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 11\n",
      "\t\t\t* corpus_2: 11\n",
      "[2024-05-29 07:17:10,663 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 12\n",
      "\t\t\t* corpus_2: 11\n",
      "[2024-05-29 07:17:13,289 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 12\n",
      "\t\t\t* corpus_2: 12\n",
      "[2024-05-29 07:17:13,431 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 13\n",
      "\t\t\t* corpus_2: 12\n",
      "[2024-05-29 07:17:16,116 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 13\n",
      "\t\t\t* corpus_2: 13\n",
      "[2024-05-29 07:17:16,263 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 14\n",
      "\t\t\t* corpus_2: 13\n",
      "[2024-05-29 07:17:19,001 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 14\n",
      "\t\t\t* corpus_2: 14\n",
      "[2024-05-29 07:17:19,149 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 15\n",
      "\t\t\t* corpus_2: 14\n",
      "[2024-05-29 07:17:22,024 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 15\n",
      "\t\t\t* corpus_2: 15\n",
      "[2024-05-29 07:17:22,227 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 16\n",
      "\t\t\t* corpus_2: 15\n",
      "[2024-05-29 07:17:25,479 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 16\n",
      "\t\t\t* corpus_2: 16\n",
      "[2024-05-29 07:17:25,632 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 17\n",
      "\t\t\t* corpus_2: 16\n",
      "[2024-05-29 07:17:28,369 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 17\n",
      "\t\t\t* corpus_2: 17\n",
      "[2024-05-29 07:17:28,515 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 18\n",
      "\t\t\t* corpus_2: 17\n",
      "[2024-05-29 07:17:31,198 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 18\n",
      "\t\t\t* corpus_2: 18\n",
      "[2024-05-29 07:17:31,351 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 19\n",
      "\t\t\t* corpus_2: 18\n",
      "[2024-05-29 07:17:34,099 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 19\n",
      "\t\t\t* corpus_2: 19\n",
      "[2024-05-29 07:17:34,246 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 20\n",
      "\t\t\t* corpus_2: 19\n",
      "[2024-05-29 07:17:37,479 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 20\n",
      "\t\t\t* corpus_2: 20\n",
      "[2024-05-29 07:17:37,628 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 21\n",
      "\t\t\t* corpus_2: 20\n",
      "[2024-05-29 07:17:40,338 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 21\n",
      "\t\t\t* corpus_2: 21\n",
      "[2024-05-29 07:17:40,487 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 22\n",
      "\t\t\t* corpus_2: 21\n",
      "[2024-05-29 07:17:43,495 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 22\n",
      "\t\t\t* corpus_2: 22\n",
      "[2024-05-29 07:17:43,706 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 23\n",
      "\t\t\t* corpus_2: 22\n",
      "[2024-05-29 07:17:47,023 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 23\n",
      "\t\t\t* corpus_2: 23\n",
      "[2024-05-29 07:17:47,177 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 24\n",
      "\t\t\t* corpus_2: 23\n",
      "[2024-05-29 07:17:49,979 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 24\n",
      "\t\t\t* corpus_2: 24\n",
      "[2024-05-29 07:17:50,127 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 25\n",
      "\t\t\t* corpus_2: 24\n",
      "[2024-05-29 07:17:52,817 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 25\n",
      "\t\t\t* corpus_2: 25\n",
      "[2024-05-29 07:17:52,969 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 26\n",
      "\t\t\t* corpus_2: 25\n",
      "[2024-05-29 07:17:55,731 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 26\n",
      "\t\t\t* corpus_2: 26\n",
      "[2024-05-29 07:17:55,885 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 27\n",
      "\t\t\t* corpus_2: 26\n",
      "[2024-05-29 07:17:58,873 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 27\n",
      "\t\t\t* corpus_2: 27\n",
      "[2024-05-29 07:17:59,114 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 28\n",
      "\t\t\t* corpus_2: 27\n",
      "[2024-05-29 07:18:02,375 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 28\n",
      "\t\t\t* corpus_2: 28\n",
      "[2024-05-29 07:18:02,528 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 29\n",
      "\t\t\t* corpus_2: 28\n",
      "[2024-05-29 07:18:05,250 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 29\n",
      "\t\t\t* corpus_2: 29\n",
      "[2024-05-29 07:18:05,397 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 30\n",
      "\t\t\t* corpus_2: 29\n",
      "[2024-05-29 07:18:08,132 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 30\n",
      "\t\t\t* corpus_2: 30\n",
      "[2024-05-29 07:18:08,279 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 31\n",
      "\t\t\t* corpus_2: 30\n",
      "[2024-05-29 07:18:11,885 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 31\n",
      "\t\t\t* corpus_2: 31\n",
      "[2024-05-29 07:18:12,117 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 32\n",
      "\t\t\t* corpus_2: 31\n",
      "[2024-05-29 07:18:15,489 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 32\n",
      "\t\t\t* corpus_2: 32\n",
      "[2024-05-29 07:18:15,643 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 33\n",
      "\t\t\t* corpus_2: 32\n",
      "[2024-05-29 07:18:18,459 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 33\n",
      "\t\t\t* corpus_2: 33\n",
      "[2024-05-29 07:18:18,613 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 34\n",
      "\t\t\t* corpus_2: 33\n",
      "[2024-05-29 07:18:21,338 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 34\n",
      "\t\t\t* corpus_2: 34\n",
      "[2024-05-29 07:18:21,485 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 35\n",
      "\t\t\t* corpus_2: 34\n",
      "[2024-05-29 07:18:24,202 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 35\n",
      "\t\t\t* corpus_2: 35\n",
      "[2024-05-29 07:18:24,353 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 36\n",
      "\t\t\t* corpus_2: 35\n",
      "[2024-05-29 07:18:27,061 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 36\n",
      "\t\t\t* corpus_2: 36\n",
      "[2024-05-29 07:18:27,210 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 37\n",
      "\t\t\t* corpus_2: 36\n",
      "[2024-05-29 07:18:29,958 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 37\n",
      "\t\t\t* corpus_2: 37\n",
      "[2024-05-29 07:18:30,107 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 38\n",
      "\t\t\t* corpus_2: 37\n",
      "[2024-05-29 07:18:33,354 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 38\n",
      "\t\t\t* corpus_2: 38\n",
      "[2024-05-29 07:18:33,506 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 39\n",
      "\t\t\t* corpus_2: 38\n",
      "[2024-05-29 07:18:36,211 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 39\n",
      "\t\t\t* corpus_2: 39\n",
      "[2024-05-29 07:18:36,363 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 40\n",
      "\t\t\t* corpus_2: 39\n",
      "[2024-05-29 07:18:39,276 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 40\n",
      "\t\t\t* corpus_2: 40\n",
      "[2024-05-29 07:18:39,515 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 41\n",
      "\t\t\t* corpus_2: 40\n",
      "[2024-05-29 07:18:42,876 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 41\n",
      "\t\t\t* corpus_2: 41\n",
      "[2024-05-29 07:18:43,032 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 42\n",
      "\t\t\t* corpus_2: 41\n",
      "[2024-05-29 07:18:45,861 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 42\n",
      "\t\t\t* corpus_2: 42\n",
      "[2024-05-29 07:18:46,018 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 43\n",
      "\t\t\t* corpus_2: 42\n",
      "[2024-05-29 07:18:48,743 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 43\n",
      "\t\t\t* corpus_2: 43\n",
      "[2024-05-29 07:18:48,892 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 44\n",
      "\t\t\t* corpus_2: 43\n",
      "[2024-05-29 07:18:51,951 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 44\n",
      "\t\t\t* corpus_2: 44\n",
      "[2024-05-29 07:18:52,173 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 45\n",
      "\t\t\t* corpus_2: 44\n",
      "[2024-05-29 07:18:55,387 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 45\n",
      "\t\t\t* corpus_2: 45\n",
      "[2024-05-29 07:18:55,542 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 46\n",
      "\t\t\t* corpus_2: 45\n",
      "[2024-05-29 07:18:58,323 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 46\n",
      "\t\t\t* corpus_2: 46\n",
      "[2024-05-29 07:18:58,477 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 47\n",
      "\t\t\t* corpus_2: 46\n",
      "[2024-05-29 07:19:01,178 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 47\n",
      "\t\t\t* corpus_2: 47\n",
      "[2024-05-29 07:19:01,325 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 48\n",
      "\t\t\t* corpus_2: 47\n",
      "[2024-05-29 07:19:04,128 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 48\n",
      "\t\t\t* corpus_2: 48\n",
      "[2024-05-29 07:19:04,281 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 49\n",
      "\t\t\t* corpus_2: 48\n",
      "[2024-05-29 07:19:07,309 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 49\n",
      "\t\t\t* corpus_2: 49\n",
      "[2024-05-29 07:19:07,467 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 50\n",
      "\t\t\t* corpus_2: 49\n",
      "[2024-05-29 07:19:10,652 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 50\n",
      "\t\t\t* corpus_2: 50\n",
      "[2024-05-29 07:19:10,904 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 51\n",
      "\t\t\t* corpus_2: 50\n",
      "[2024-05-29 07:19:14,405 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 51\n",
      "\t\t\t* corpus_2: 51\n",
      "[2024-05-29 07:19:14,563 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 52\n",
      "\t\t\t* corpus_2: 51\n",
      "[2024-05-29 07:19:17,393 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 52\n",
      "\t\t\t* corpus_2: 52\n",
      "[2024-05-29 07:19:17,552 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 53\n",
      "\t\t\t* corpus_2: 52\n",
      "[2024-05-29 07:19:20,317 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 53\n",
      "\t\t\t* corpus_2: 53\n",
      "[2024-05-29 07:19:20,471 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 54\n",
      "\t\t\t* corpus_2: 53\n",
      "[2024-05-29 07:19:23,219 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 54\n",
      "\t\t\t* corpus_2: 54\n",
      "[2024-05-29 07:19:23,373 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 55\n",
      "\t\t\t* corpus_2: 54\n",
      "[2024-05-29 07:19:26,341 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 55\n",
      "\t\t\t* corpus_2: 55\n",
      "[2024-05-29 07:19:26,580 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 56\n",
      "\t\t\t* corpus_2: 55\n",
      "[2024-05-29 07:19:30,538 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 56\n",
      "\t\t\t* corpus_2: 56\n",
      "[2024-05-29 07:19:30,709 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 57\n",
      "\t\t\t* corpus_2: 56\n",
      "[2024-05-29 07:19:34,110 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 57\n",
      "\t\t\t* corpus_2: 57\n",
      "[2024-05-29 07:19:34,265 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 58\n",
      "\t\t\t* corpus_2: 57\n",
      "[2024-05-29 07:19:37,719 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 58\n",
      "\t\t\t* corpus_2: 58\n",
      "[2024-05-29 07:19:37,879 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 59\n",
      "\t\t\t* corpus_2: 58\n",
      "[2024-05-29 07:19:41,168 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 59\n",
      "\t\t\t* corpus_2: 59\n",
      "[2024-05-29 07:19:41,406 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 60\n",
      "\t\t\t* corpus_2: 59\n",
      "[2024-05-29 07:19:45,268 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 60\n",
      "\t\t\t* corpus_2: 60\n",
      "[2024-05-29 07:19:45,424 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 61\n",
      "\t\t\t* corpus_2: 60\n",
      "[2024-05-29 07:19:48,490 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 61\n",
      "\t\t\t* corpus_2: 61\n",
      "[2024-05-29 07:19:48,741 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 62\n",
      "\t\t\t* corpus_2: 61\n",
      "[2024-05-29 07:19:52,087 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 62\n",
      "\t\t\t* corpus_2: 62\n",
      "[2024-05-29 07:19:52,441 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 63\n",
      "\t\t\t* corpus_2: 62\n",
      "[2024-05-29 07:19:55,946 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 63\n",
      "\t\t\t* corpus_2: 63\n",
      "[2024-05-29 07:19:56,109 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 64\n",
      "\t\t\t* corpus_2: 63\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Shen\\miniconda3\\envs\\TAA\\lib\\runpy.py\", line 194, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\Shen\\miniconda3\\envs\\TAA\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\Shen\\miniconda3\\envs\\TAA\\Scripts\\onmt_train.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\Shen\\miniconda3\\envs\\TAA\\lib\\site-packages\\onmt\\bin\\train.py\", line 67, in main\n",
      "    train(opt)\n",
      "  File \"C:\\Users\\Shen\\miniconda3\\envs\\TAA\\lib\\site-packages\\onmt\\bin\\train.py\", line 52, in train\n",
      "    train_process(opt, device_id=0)\n",
      "  File \"C:\\Users\\Shen\\miniconda3\\envs\\TAA\\lib\\site-packages\\onmt\\train_single.py\", line 238, in main\n",
      "    trainer.train(\n",
      "  File \"C:\\Users\\Shen\\miniconda3\\envs\\TAA\\lib\\site-packages\\onmt\\trainer.py\", line 308, in train\n",
      "    for i, (batches, normalization) in enumerate(self._accum_batches(train_iter)):\n",
      "  File \"C:\\Users\\Shen\\miniconda3\\envs\\TAA\\lib\\site-packages\\onmt\\trainer.py\", line 238, in _accum_batches\n",
      "    for batch, bucket_idx in iterator:\n",
      "  File \"C:\\Users\\Shen\\miniconda3\\envs\\TAA\\lib\\site-packages\\onmt\\inputters\\dynamic_iterator.py\", line 373, in __iter__\n",
      "    for (tensor_batch, bucket_idx) in self.data_iter:\n",
      "  File \"C:\\Users\\Shen\\miniconda3\\envs\\TAA\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 633, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"C:\\Users\\Shen\\miniconda3\\envs\\TAA\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1345, in _next_data\n",
      "    return self._process_data(data)\n",
      "  File \"C:\\Users\\Shen\\miniconda3\\envs\\TAA\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1371, in _process_data\n",
      "    data.reraise()\n",
      "  File \"C:\\Users\\Shen\\miniconda3\\envs\\TAA\\lib\\site-packages\\torch\\_utils.py\", line 644, in reraise\n",
      "    raise exception\n",
      "MemoryError: Caught MemoryError in DataLoader worker process 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Shen\\miniconda3\\envs\\TAA\\lib\\site-packages\\onmt\\inputters\\dynamic_iterator.py\", line 85, in __iter__\n",
      "    item = next(iterator)\n",
      "StopIteration\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Shen\\miniconda3\\envs\\TAA\\lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 308, in _worker_loop\n",
      "    data = fetcher.fetch(index)\n",
      "  File \"C:\\Users\\Shen\\miniconda3\\envs\\TAA\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 41, in fetch\n",
      "    data = next(self.dataset_iter)\n",
      "  File \"C:\\Users\\Shen\\miniconda3\\envs\\TAA\\lib\\site-packages\\onmt\\inputters\\dynamic_iterator.py\", line 341, in __iter__\n",
      "    for bucket, bucket_idx in self._bucketing():\n",
      "  File \"C:\\Users\\Shen\\miniconda3\\envs\\TAA\\lib\\site-packages\\onmt\\inputters\\dynamic_iterator.py\", line 275, in _bucketing\n",
      "    for ex in self.mixer:\n",
      "  File \"C:\\Users\\Shen\\miniconda3\\envs\\TAA\\lib\\site-packages\\onmt\\inputters\\dynamic_iterator.py\", line 89, in __iter__\n",
      "    item = next(iterator)\n",
      "  File \"C:\\Users\\Shen\\miniconda3\\envs\\TAA\\lib\\site-packages\\onmt\\inputters\\text_corpus.py\", line 281, in __iter__\n",
      "    yield from corpus\n",
      "  File \"C:\\Users\\Shen\\miniconda3\\envs\\TAA\\lib\\site-packages\\onmt\\inputters\\text_corpus.py\", line 242, in _process\n",
      "    example[\"src_original\"] = example[\"src_original\"].strip().split(\" \")\n",
      "MemoryError\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!onmt_train -config run2_en_fr.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!onmt_translate -model ../data/model/run2/model_step_1000.pt -src ../data/raw/Europarl/Europarl_test_500.en -output ../data/result/pred2_Europarl_1000.txt -gpu 0 -verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!onmt_translate -model ../data/model/run2/model_step_1000.pt -src ../data/raw/Emea/Emea_test_500.en -output ../data/result/pred2_Emea_1000.txt -gpu 0 -verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/result/pred2_Europarl_1000.txt\", \"r\", encoding=\"utf-8\") as pred_file:\n",
    "    translations = pred_file.readlines()\n",
    "with open(\"../data/raw/Europarl/Europarl_test_500.fr\", \"r\", encoding=\"utf-8\") as ref_file:\n",
    "    references = ref_file.readlines()\n",
    "references = [ref.strip().split() for ref in references]\n",
    "translations = [trans.strip().split() for trans in translations]\n",
    "bleu_score = nltk.translate.bleu_score.corpus_bleu(references, translations, smoothing_function=nltk.translate.bleu_score.SmoothingFunction().method1)\n",
    "print(\"BLEU Score run 2 Europarl:\", bleu_score)\n",
    "\n",
    "\n",
    "with open(\"../data/result/pred2_Emea_1000.txt\", \"r\", encoding=\"utf-8\") as pred_file:\n",
    "    translations = pred_file.readlines()\n",
    "with open(\"../data/raw/Emea/Emea_test_500.fr\", \"r\", encoding=\"utf-8\") as ref_file:\n",
    "    references = ref_file.readlines()\n",
    "references = [ref.strip().split() for ref in references]\n",
    "translations = [trans.strip().split() for trans in translations]\n",
    "bleu_score = nltk.translate.bleu_score.corpus_bleu(references, translations, smoothing_function=nltk.translate.bleu_score.SmoothingFunction().method1)\n",
    "print(\"BLEU Score run 2 Emea:\", bleu_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TAA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
